{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of ECG Diagnoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Benchmarking Paper: &nbsp; [Deep Learning for ECG Analysis: Benchmarks and Insights from PTB-XL](https://ieeexplore.ieee.org/document/9190034)\n",
    "\n",
    "Original Repository from Paper: &nbsp; [Repository on GitHub](https://github.com/helme/ecg_ptbxl_benchmarking)\n",
    "\n",
    "Dataset: &nbsp; [PTB-XL, a large publicly available electrocardiography dataset](https://physionet.org/content/ptb-xl/1.0.3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import utils\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix\n",
    "import torch\n",
    "import fastai\n",
    "import wfdb\n",
    "import pickle\n",
    "\n",
    "from models.fastai_model import fastai_model\n",
    "\n",
    "from IPython.display import Image\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Before finetuning a pretrained model of the experiments we provide in our repository (or precomputed and provided [here](https://datacloud.hhi.fraunhofer.de/nextcloud/s/NCjYws3mamLrkKq)), first load your custom 100 Hz sampled 12-lead ECG signal data `X` of shape `[N,L,12]` in Millivolts (mV) and multi-hot encoded labels `y` of shape `[N,C]` as numpy arrays, where `C` is the number of classes and `N` the number of total samples in this dataset. Although PTB-XL comes with fixed `L=1000` (i,e. 10 seconds), it is not required to be fixed, **BUT** the shortest sample must be longer than `input_size` of the specific model (e.g. 2.5 seconds for our fastai-models).\n",
    "\n",
    "For proper tinetuning split your data into four numpy arrays: `X_train`,`y_train`,`X_val` and `y_val`\n",
    "\n",
    "Example: finetune model trained on all (71) on superdiagnostic (5)\n",
    "\n",
    "\n",
    "Below we provide an example for loading [PTB-XL](https://physionet.org/content/ptb-xl/1.0.1/) aggregated at the `superdiagnostic` level, where we use the provided folds for train-validation-split:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'exp0' # exp0. exp1.1, exp1.1.1\n",
    "sampling_frequency = 100\n",
    "datafolder = '../../data/ptbxl/'\n",
    "task = 'all'\n",
    "outputfolder_for_mlb = f'../output/{experiment}/'\n",
    "\n",
    "# Load PTB-XL data\n",
    "df, raw_labels = utils.load_dataset(datafolder, sampling_frequency)\n",
    "\n",
    "# Preprocess label data\n",
    "labels =  utils.compute_label_aggregations(raw_labels, datafolder, task)\n",
    "\n",
    "# Select relevant data and convert to one-hot\n",
    "X, Y, y, mlb =  utils.select_data(df, labels, task, min_samples=0, output_folder=outputfolder_for_mlb)\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor classes for prediction (label manipulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_class_names = list(mlb.classes_)\n",
    "all_class_ids = [i for i, name in enumerate(all_class_names)]\n",
    "num_all_classes = len(all_class_names)\n",
    "\n",
    "print(f\"All classes: {all_class_names}\")\n",
    "print(f'Class IDs: {all_class_ids}')\n",
    "print(f\"Count classes: {num_all_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes_1 = ['MI'] # OTHER\n",
    "# classes_2 = ['IMI'] # OTHER\n",
    "# classes_3 = ['AMI'] # OTHER\n",
    "# classes_4 = ['IMI', 'AMI'] # OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract excisting classes\n",
    "# classes = classes_2\n",
    "# class_ids = [i for i, name in enumerate(all_class_names) if name in classes]\n",
    "# num_classes = len(classes)\n",
    "\n",
    "# print(f\"Classes to predict: {classes}\")\n",
    "# print(f'Class IDs: {class_ids}')\n",
    "# print(f\"Count classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add new class\n",
    "# new_classes = classes + ['OTHER']\n",
    "# other_id = num_classes # ids start at zero, so we can use num_classes directly\n",
    "# new_class_ids = class_ids + [other_id]\n",
    "# num_new_classes = len(new_classes)\n",
    "\n",
    "\n",
    "# print(f\"Classes to predict: {new_classes}\")\n",
    "# print(f'Class IDs: {new_class_ids}')\n",
    "# print(f\"Count classes: {num_new_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # label matrix containing zeros\n",
    "# y_new = np.zeros((y.shape[0], num_new_classes))\n",
    "\n",
    "# # fill new label matrix with existing classes\n",
    "# for i, class_id in enumerate(class_ids):\n",
    "#     y_new[:, i] = y[:, class_id]\n",
    "\n",
    "# display(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # true when at least one target class is present, false otherwise\n",
    "# target_mask = y[:, class_ids].sum(axis=1) > 0\n",
    "\n",
    "# # true when at least one label is present, false otherwise\n",
    "# any_label_mask = y.sum(axis=1) > 0\n",
    "\n",
    "# # fill new class with 1s where target classes are present\n",
    "# other_mask = (~target_mask) & any_label_mask\n",
    "\n",
    "# # set OTHER label to 1 where target classes are not present\n",
    "# y_new[other_mask, other_id] = 1\n",
    "\n",
    "# print(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of old labels:\")\n",
    "\n",
    "class_names = list(mlb.classes_)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "percentage_all_samples = 0\n",
    "for i, class_name in enumerate(class_names):\n",
    "    count = (y[:, i] == 1).sum()\n",
    "    percentage = count / y.shape[0] * 100 if y.shape[0] > 0 else 0\n",
    "    percentage_all_samples += percentage\n",
    "    print(f\"{class_name}: {count} Samples ({percentage:.1f}%)\")\n",
    "\n",
    "print(f'∑ Samples: {y.shape[0]} ({percentage_all_samples:.1f}%) # Note: >100% due to multi-label')\n",
    "print(f\"\\nSamples without any label: {(y.sum(axis=1) == 0).sum():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nDistribution of new labels:\")\n",
    "\n",
    "# percentage_all_samples = 0\n",
    "# for i, class_name in enumerate(new_classes):\n",
    "#     count = (y_new[:, i] == 1).sum()\n",
    "#     percentage = count / y_new.shape[0] * 100\n",
    "#     percentage_all_samples += percentage\n",
    "#     print(f\"{class_name}: {count} Samples ({percentage:.1f}%)\")\n",
    "\n",
    "# print(f'\\n∑ Samples: {y_new.shape[0]} ({percentage_all_samples:.1f})%')\n",
    "# print(f\"∑ Samples without any label: {(y_new.sum(axis=1) == 0).sum():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X.shape[1:] # shape of samples, [None, 12] in case of different lengths\n",
    "print(f'Input shape {input_shape} should be the same as (1000, 12)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update variables\n",
    "class_names = all_class_names # new_classes \n",
    "num_classes = num_all_classes # num_new_classes # <=== number of classes in the finetuning dataset\n",
    "#y = y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation and Testing Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As written on physionet:\n",
    "\n",
    "Cross-validation Folds: recommended 10-fold train-test splits (strat_fold) obtained via stratified sampling while respecting patient assignments, i.e. all records of a particular patient were assigned to the same fold. Records in fold 9 and 10 underwent at least one human evaluation and are therefore of a particularly high label quality. We therefore propose to use folds 1-8 as training set, fold 9 as validation set and fold 10 as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-8 for training \n",
    "X_train = X[Y.strat_fold < 9]\n",
    "y_train = y[Y.strat_fold < 9]\n",
    "\n",
    "# 9 for validation\n",
    "X_val = X[Y.strat_fold == 9]\n",
    "y_val = y[Y.strat_fold == 9]\n",
    "\n",
    "# 10 for testing\n",
    "X_test = X[Y.strat_fold == 10]\n",
    "y_test = y[Y.strat_fold == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = X_train.shape[0] + X_val.shape[0] + X_test.shape[0]\n",
    "\n",
    "split_data = {\n",
    "    'Split': ['Train', 'Validation', 'Test', 'Total'],\n",
    "    'X Shape': [str(X_train.shape), str(X_val.shape), str(X_test.shape), f\"({total_samples}, {X.shape[1]}, {X.shape[2]})\"],\n",
    "    'y Shape': [str(y_train.shape), str(y_val.shape), str(y_test.shape), f\"({total_samples}, {y.shape[1]})\"],\n",
    "    'Samples': [X_train.shape[0], X_val.shape[0], X_test.shape[0], total_samples],\n",
    "    'Percentage': [f\"{X_train.shape[0]/total_samples*100:.1f}%\", \n",
    "                   f\"{X_val.shape[0]/total_samples*100:.1f}%\", \n",
    "                   f\"{X_test.shape[0]/total_samples*100:.1f}%\", \n",
    "                   f\"{(X_train.shape[0] + X_val.shape[0] + X_test.shape[0])/total_samples*100:.1f}%\"]\n",
    "}\n",
    "\n",
    "df_split = pd.DataFrame(split_data)\n",
    "#print(tabulate(df_split, headers='keys', tablefmt='fancy_grid'))\n",
    "display(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "total_samples_all = y_train.shape[0] + y_val.shape[0] + y_test.shape[0]\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    train_count = (y_train[:, i] == 1).sum()\n",
    "    val_count = (y_val[:, i] == 1).sum()\n",
    "    test_count = (y_test[:, i] == 1).sum()\n",
    "    total_count = train_count + val_count + test_count\n",
    "    \n",
    "    train_pct = train_count / y_train.shape[0] * 100\n",
    "    val_pct = val_count / y_val.shape[0] * 100\n",
    "    test_pct = test_count / y_test.shape[0] * 100\n",
    "    total_pct = total_count / total_samples_all * 100\n",
    "    \n",
    "    data.append({\n",
    "        'Class': class_name,\n",
    "        'Train Count': train_count,\n",
    "        'Train %': f\"{train_pct:.1f}%\",\n",
    "        'Val Count': val_count,\n",
    "        'Val %': f\"{val_pct:.1f}%\",\n",
    "        'Test Count': test_count,\n",
    "        'Test %': f\"{test_pct:.1f}%\",\n",
    "        'Total Count': total_count,\n",
    "        'Total %': f\"{total_pct:.1f}%\"\n",
    "    })\n",
    "\n",
    "# Add Total row\n",
    "total_train = sum([(y_train[:, i] == 1).sum() for i in range(len(class_names))])\n",
    "total_val = sum([(y_val[:, i] == 1).sum() for i in range(len(class_names))])\n",
    "total_test = sum([(y_test[:, i] == 1).sum() for i in range(len(class_names))])\n",
    "total_all = total_train + total_val + total_test\n",
    "\n",
    "total_train_pct = total_train / y_train.shape[0] * 100\n",
    "total_val_pct = total_val / y_val.shape[0] * 100\n",
    "total_test_pct = total_test / y_test.shape[0] * 100\n",
    "total_all_pct = total_all / total_samples_all * 100\n",
    "\n",
    "data.append({\n",
    "    'Class': 'TOTAL',\n",
    "    'Train Count': total_train,\n",
    "    'Train %': f\"{total_train_pct:.1f}%\",\n",
    "    'Val Count': total_val,\n",
    "    'Val %': f\"{total_val_pct:.1f}%\",\n",
    "    'Test Count': total_test,\n",
    "    'Test %': f\"{total_test_pct:.1f}%\",\n",
    "    'Total Count': total_all,\n",
    "    'Total %': f\"{total_all_pct:.1f}%\"\n",
    "})\n",
    "\n",
    "df_class_distribution = pd.DataFrame(data)\n",
    "#print(tabulate(df_class_distribution, headers='keys', tablefmt='fancy_grid'))\n",
    "display(df_class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Standardizer\n",
    "\n",
    "Since we standardize inputs to zero mean and unit variance, your custom data needs to be standardized with the respective mean and variance. This is also provided in the respective experiment folder `output/expX/data/standard_scaler.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = pickle.load(open(f'../output/{experiment}/data/standard_scaler.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = utils.apply_standardizer(X_train, standard_scaler)\n",
    "X_val = utils.apply_standardizer(X_val, standard_scaler)\n",
    "X_test = utils.apply_standardizer(X_test, standard_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'fastai_xresnet1d101'\n",
    "pretrainedfolder = '../output/'+experiment+'/models/'+modelname+'/'\n",
    "outputfolder = '../output/'\n",
    "\n",
    "if experiment=='exp0':\n",
    "    n_classes_pretrained = 71\n",
    "elif experiment=='exp1.1':\n",
    "    n_classes_pretrained = 23\n",
    "elif experiment=='exp1.1.1':\n",
    "    n_classes_pretrained = 5\n",
    "else:\n",
    "    print(f'Experiment {experiment} not supported for this code snippet.')\n",
    "\n",
    "\n",
    "print(f'Pretrainedfolder: {pretrainedfolder}')\n",
    "print(f'Outputfolder: {outputfolder}')\n",
    "print(f'Classes in pretrained model: {n_classes_pretrained}')\n",
    "print(f'Classes in finetuning dataset: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fastai_model(\n",
    "    modelname, \n",
    "    n_classes = num_classes, # <=== number of classes in the finetuning dataset\n",
    "    freq = sampling_frequency, \n",
    "    outputfolder = pretrainedfolder, \n",
    "    input_shape = input_shape, \n",
    "    loss = 'cross_entropy',\n",
    "    pretrainedfolder = pretrainedfolder,\n",
    "    n_classes_pretrained = n_classes_pretrained,\n",
    "    pretrained = True,\n",
    "    epochs_finetuning = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FASTAI MODEL PARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Model name: {model.name}\")\n",
    "print(f\"Number of classes: {model.num_classes}\")\n",
    "print(f\"Input shape: {model.input_shape}\")\n",
    "print(f\"Input size: {model.input_size}\")\n",
    "print(f\"Input channels: {model.input_channels}\")\n",
    "print(f\"Target frequency: {model.target_fs}\")\n",
    "print(f\"Batch size: {model.bs}\")\n",
    "\n",
    "print(f\"\\nTraining parameters:\")\n",
    "print(f\"Learning rate: {model.lr}\")\n",
    "print(f\"Weight decay: {model.wd}\")\n",
    "print(f\"Loss function: {model.loss}\")\n",
    "print(f\"Epochs: {model.epochs}\")\n",
    "print(f\"Epochs finetuning: {model.epochs_finetuning}\")\n",
    "\n",
    "print(f\"\\nPretrained model parameters:\")\n",
    "print(f\"Pretrained classes: {model.n_classes_pretrained}\")\n",
    "print(f\"Pretrained folder: {model.pretrainedfolder}\")\n",
    "\n",
    "print(f\"\\nData processing parameters:\")\n",
    "print(f\"Chunk length train: {model.chunk_length_train}\")\n",
    "print(f\"Chunk length valid: {model.chunk_length_valid}\")\n",
    "print(f\"Stride length train: {model.stride_length_train}\")\n",
    "print(f\"Stride length valid: {model.stride_length_valid}\")\n",
    "\n",
    "# Wichtigste Überprüfung: Stimmen die Klassen überein?\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASS COMPATIBILITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Your data classes: {num_classes}\")\n",
    "print(f\"Model classes: {model.num_classes}\")\n",
    "print(f\"Pretrained classes: {model.n_classes_pretrained}\")\n",
    "\n",
    "if model.num_classes == num_classes:\n",
    "    print(\"✓ Model classes match your data classes\")\n",
    "elif model.num_classes == model.n_classes_pretrained:\n",
    "    print(\"⚠️ Model still has pretrained number of classes\")\n",
    "    print(\"This might be intentional if you're using transfer learning\")\n",
    "else:\n",
    "    print(\"✗ Model classes do NOT match your data or pretrained classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune model\n",
    "\n",
    "Calling `model.fit` of a model with `pretrained=True` will perform finetuning as proposed in our work i.e. **gradual unfreezing and discriminative learning rates**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict(X_val)\n",
    "#utils.evaluate_experiment(y_val, y_val_pred)\n",
    "\n",
    "results = utils.generate_results(np.arange(len(y_val)), y_true=y_val, y_pred=y_val_pred, thresholds=None)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_val_binary = y_val.astype(int)\n",
    "y_pred_binary = (y_val_pred > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_confusion_matrices(y_true_binary, y_pred_binary, class_names, selected_labels=None, figsize_per_plot=(4, 3)):\n",
    "    \"\"\"\n",
    "    Erstellt Confusion Matrix Heatmaps für ausgewählte Labels\n",
    "    \n",
    "    Parameters:\n",
    "    y_true_binary: Binary true labels\n",
    "    y_pred_binary: Binary predicted labels  \n",
    "    class_names: Liste aller Klassennamen\n",
    "    selected_labels: Liste der gewünschten Labels (None = alle Labels)\n",
    "    figsize_per_plot: Größe pro einzelner Matrix (width, height)\n",
    "    \"\"\"\n",
    "    \n",
    "    if selected_labels is None:\n",
    "        selected_labels = class_names\n",
    "    \n",
    "    # Validierung der Labels\n",
    "    invalid_labels = [label for label in selected_labels if label not in class_names]\n",
    "    if invalid_labels:\n",
    "        print(f\"Warnung: Folgende Labels sind nicht verfügbar: {invalid_labels}\")\n",
    "        selected_labels = [label for label in selected_labels if label in class_names]\n",
    "    \n",
    "    if len(selected_labels) == 0:\n",
    "        print(\"Keine gültigen Labels ausgewählt!\")\n",
    "        return\n",
    "    \n",
    "    # Dynamische Berechnung der Grid-Größe\n",
    "    import math\n",
    "    num_selected = len(selected_labels)\n",
    "    cols = min(6, num_selected)  # Maximal 6 Spalten\n",
    "    rows = math.ceil(num_selected / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * figsize_per_plot[0], rows * figsize_per_plot[1]))\n",
    "    \n",
    "    # Flatten axes array für einfacheren Zugriff\n",
    "    if num_selected == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, label in enumerate(selected_labels):\n",
    "        # Index des Labels finden\n",
    "        label_idx = class_names.index(label)\n",
    "        \n",
    "        # Confusion Matrix berechnen\n",
    "        cm = confusion_matrix(y_true_binary[:, label_idx], y_pred_binary[:, label_idx])\n",
    "        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "        \n",
    "        # Kombiniere absolute Zahlen und Prozentzahlen\n",
    "        labels = np.asarray([f'{v1}\\n({v2:.1f}%)' \n",
    "                            for v1, v2 in zip(cm.flatten(), cm_percent.flatten())]).reshape(2,2)\n",
    "        \n",
    "        # Heatmap erstellen\n",
    "        sns.heatmap(cm, annot=labels, fmt='', ax=axes[i], cmap='Blues', \n",
    "                   cbar_kws={'shrink': 0.8})\n",
    "        axes[i].set_title(f'{label}', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Predicted', fontsize=10)\n",
    "        axes[i].set_ylabel('Actual', fontsize=10)\n",
    "        \n",
    "        # Achsenbeschriftungen für bessere Lesbarkeit\n",
    "        axes[i].set_xticklabels(['Negative', 'Positive'], rotation=0)\n",
    "        axes[i].set_yticklabels(['Negative', 'Positive'], rotation=0)\n",
    "    \n",
    "    # Verstecke leere Subplots\n",
    "    for j in range(num_selected, rows * cols):\n",
    "        if j < len(axes):\n",
    "            axes[j].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'Confusion Matrices', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Nur bestimmte Labels auswählen\n",
    "selected_labels = ['NORM', 'IMI', 'AMI', 'PMI', 'LMI']\n",
    "print(f\"Ausgewählte Labels: {selected_labels}\")\n",
    "plot_confusion_matrices(y_val_binary, y_pred_binary, class_names, selected_labels)\n",
    "\n",
    "# Alle Labels\n",
    "print(\"\\nAlle Labels:\")\n",
    "plot_confusion_matrices(y_val_binary, y_pred_binary, class_names, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_val[:, i], y_val_pred[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('True Positive Rate (Sensitivity/Recall): TP / (TP + FN)')\n",
    "plt.ylabel('False Positive Rate (Specificity): FP / (FP + TN)')\n",
    "plt.title('ROC Curves for All Classes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_benchmarking_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
